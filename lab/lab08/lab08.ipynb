{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Bayesian and Frequentist Takes on Multi-Armed Bandits\n",
    "Welcome to the eighth DS102 lab! \n",
    "\n",
    "The goals of this lab is to implement and gain a better understanding of the pros and cons of the Upper Confidence Bounds and Thompson Sampling algorithms\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Tuesday November 5, 2019 at 11:59 PM.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write collaborator names here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from matplotlib.widgets import Button, CheckButtons\n",
    "from matplotlib import gridspec\n",
    "import functools\n",
    "from Bandit_env import BanditEnv, Interactive_UCB_Algorithm,Interactive_TS_Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means=[10,9.5,8.5,7.5,7.0,6.5]\n",
    "variance=1.2\n",
    "standard_deviations=[np.sqrt(variance) for arm in range(len(means))]\n",
    "bandit_env=BanditEnv(means,standard_deviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "\n",
    "In this lab we will be implementing two of the most common approaches to solving stochastic Multi-Armed Bandit (MAB) problems. We first define the problem and then you will have a chance to implement at the Upper Confidence Bounds and Thompson Sampling algorithms from lecture and analyze their performance.\n",
    "\n",
    "## Setup:\n",
    "A MAB problem is a simple setting in which it is easy to analyze the __exploration/exploitation__ tradeoff that is extremely common in machine learning. The setup is as follows:\n",
    "\n",
    "A MAB instance is a set $\\mathcal{A}$ of $K$ arms. Each arm $a \\in \\mathcal{A}$ is associated with a reward distribution $P_a$ which is unique to that arm. The mean of an arm $a \\in \\mathcal{A}$ is denoted as $\\mu_a=\\mathbb{E}_{P_a}[X_a]$. \n",
    "\n",
    "At each time $t=1,2,...$, an algorithm that is interacting with a MAB must choose an arm, $A_t \\in \\mathcal{A}$. The algorithm then receives a reward $X_{A_t,t}$ sampled independently from $P_{A_t}$.\n",
    "\n",
    "The __goal__ of an algorithm interacting with a MAB instance is to find the arm $A^\\ast$ with the highest mean reward $\\mu^\\ast$ as fast as possible while maintaining performance. That is, it would like to find the arm $A^\\ast$ such that:\n",
    "\n",
    "$$ A^\\ast=arg\\max_{a \\in \\mathcal{A}} \\mu_a$$\n",
    "\n",
    "where $\\mu^*=\\mu_{A^\\ast}$.\n",
    "\n",
    "This is often encoded as wanting to find an algorithm that minimizes the __regret__ over a time horizon $n$. Intuitievly, the regret is the best the algorithm could have done in hindsight if it had known which was the optimal arm. The regret of an algorithm is defined as:\n",
    "\n",
    "$$ Regret(n)= \\sum_{t=1}^n X_{A^\\ast,t} -X_{A_t,t}$$\n",
    "\n",
    "\n",
    "Most of the time, it is simpler to analyze the __pseudo-regret__, which is the mean of the regret.\n",
    "\n",
    "$$ R(n)= n \\mu^* - \\mathbb{E}\\left[\\sum_{t=1}^n X_{A_t,t}\\right]$$\n",
    "\n",
    "### Lab setup:\n",
    "In this lab, the MAB instances will have a set of arms numbered $0,1,...,K-1$. Each arm $a=0,1,...,K-1$ is associated with a Gaussian reward distribution with mean $\\mu_a$ and standard deviation of $\\sigma_a=1.2$. To be able to analyze the various algorithms, the optimal arm $A^\\ast$ will always be arm $0$, and its mean will always be $\\mu^\\ast=10$.\n",
    "\n",
    "By running the following cell, you can interact with a MAB instance of the type we will be using in this lab. You can see the reward distributions as well as the expected cumulative regret you incur when pulling each arm. \n",
    "\n",
    "Verify for yourself that explore-then-commit strategies can get stuck pulling the wrong arm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an interactive bandit instance.\n",
    "#     - Pull an arm by clickling on the colored button\n",
    "#     - The true means of the distributions are shown with the dashed horizontal lines\n",
    "#     - Large solid circle is the sample mean of the arm\n",
    "#     - Small empty circle is a sample from the arm\n",
    "#     - The reward distribution of each arm is shown on the right and can be toggled on/off by checking the box\n",
    "#     - Running Pseudo-regret is shown on the bottom and can be toggled on/off by checking the box\n",
    "\n",
    "# You may need to rerun this cell to restart the gui\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize']=[9,8]\n",
    "bandit_env.run_Interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  The Frequentist Approach: Upper Confidence Bounds\n",
    "\n",
    "The first algorithm we will analyze is the frequentist take on multi-armed bandits, known as the Upper Confidence Bounds algorithm. \n",
    "\n",
    "For each arm $a \\in \\{ 0,1,...,K-1\\}$, you keep track of:\n",
    "\n",
    "1. $T_a(t)$: the number of times arm $a$ has been pulled up to and including iteration $t$.\n",
    "2. $X_{a,1},...,X_{a,T_a(t)}$: the samples you have received from arm $a$.\n",
    "\n",
    "Using this information, you compute an upper confidence bound, $C_a(T_a(t),\\delta)$ that encompasses the true mean $\\mu_a$ with probability at least $1-\\delta$. $C_a(T_a(t),\\delta)$, must therefore satisfy:\n",
    "\n",
    "$$ \\mathbb{P}\\bigg(\\mu_a < C_a(T_a(t),\\delta)\\bigg) > 1 - \\delta.$$\n",
    "\n",
    "Note that since $C_a(n_a(t),\\delta)$ is a function of the samples $X_{a,1},...,X_{a,T_a(t)}$, it is a random variable, and we set the confidence bounds after $0$ samples to $\\infty$. That is:\n",
    "\n",
    "$$C_a(0,\\delta)=\\infty.$$\n",
    "\n",
    "The algorithm then pulls, at each round $t$, the arm with the highest upper confidence bound:\n",
    "\n",
    "$$ A_t=\\underset{a \\in \\{ 0,1,...,K-1\\}}{\\operatorname{argmax}} C_a(T_a(t),\\delta),$$\n",
    "\n",
    "where ties are broken arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Chebyshev-UCB\n",
    "\n",
    "In this first part, we will use the Chebyshev bound to construct the Upper Confidence Bound. This will result in an algorithm that we will call Chebyshev-UCB. \n",
    "\n",
    "From the last discussion, we derived the following upper confidence bound for the mean:\n",
    "\n",
    "$$ C_a(T_a(t),\\delta)= \\hat{\\mu}_a(t)+\\sqrt{\\frac{\\sigma^2}{T_a(t)\\delta}},$$\n",
    "\n",
    "where $\\hat{\\mu}_a$ is the current sample mean for arm $a$:\n",
    "\n",
    "$$ \\hat{\\mu}_a =\\frac{1}{T_a(t)} \\sum_{i=1}^{T_a(t)} X_{a,i}$$\n",
    "\n",
    "For this section, we will choose a time-varying $\\delta$ to ensure that each arm will always be pulled:\n",
    "\n",
    "$$ \\delta(t)=\\frac{1}{t^2},$$\n",
    "\n",
    "resulting in the confidence bound:\n",
    "\n",
    "$$C_a(T_a(t),\\delta(t))= \\begin{cases} \\qquad \\qquad \\infty \\ \\qquad \\qquad &: T_a(t)=0 \\\\ \\hat{\\mu}_a(t)+\\sqrt{\\frac{t^2\\sigma^2}{T_a(t)}} \\ \\ \\ \\ \\ \\  &: T_a(t)>0\\end{cases}$$\n",
    "\n",
    "Using this form of the upper confidence bound, fill out the following function which returns the choice of arm as well as the upper confidence bounds of each arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_pull_arm(t,variance,times_pulled,rewards):\n",
    "    \"\"\" \n",
    "    Implement the choice of arm for the Chebyshev-UCB algorithm\n",
    "    \n",
    "    Inputs:\n",
    "    t                  - iteration of the bandit algorithm\n",
    "    variance           - variance of all the arms\n",
    "    times_pulled       - a list of length K (where K is the number of arms) of the number of \n",
    "                         times each arm has been pulled\n",
    "    rewards            - a list of K lists. Each of the K lists holds the samples received from pulling each arm up \n",
    "                         to iteration t. \n",
    "    \n",
    "    Returns:\n",
    "    arm                -  integer representing the arm that the chebyshev-UCB algorithm would choose.\n",
    "    confidence bounds  -  a list of the chebyshev-dervied upper confidence bounds for each arm\n",
    "    \"\"\"\n",
    "    K=len(times_pulled)\n",
    "    delta=1.0/t**2\n",
    "    \n",
    "    confidence_bounds=[] #TODO\n",
    "    \n",
    "    arm=#TODO\n",
    "    \n",
    "    return arm , confidence_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function you have filled out, let us investigate the pseudo-regret of the chebyshev-UCB algorithm. Since the pseudo-regret is an expectation of the regret, the following cell runs the algorithm $20$ times anc computes the average pseudo-regret across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the figure and size of the figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "# Define the time horizon of each run, and the number of runs of each the algorithm.\n",
    "T=1000\n",
    "num_runs=20\n",
    "\n",
    "# Initialize pseudo-regret\n",
    "chebyshev_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    # re-initialize bandit environment and counters\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        # choose the arm using the chebyshev-UCB algorithm\n",
    "        arm,confidence_bounds=chebyshev_pull_arm(t,variance,bandit_env.times_pulled,bandit_env.rewards)\n",
    "        \n",
    "        # pull the arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #keep track of pseudo-regret\n",
    "    chebyshev_pseudo_regret+=np.array(bandit_env.regret)\n",
    "\n",
    "plt.plot(chebyshev_pseudo_regret/num_runs)\n",
    "plt.xlabel('#TODO')\n",
    "plt.ylabel('#TODO')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. UCB\n",
    "We will now implement the classic version of the UCB algorithm which uses Chernoff/Hoeffding bounds. \n",
    "\n",
    "Recall that for the sample mean of $n$ Gaussian random variables with variance $\\sigma^2$, the Chernoff bound is of the form:\n",
    "\n",
    "$$ \\mathbb{P}(\\hat{\\mu}_a-\\mu_a > \\epsilon) < e^{-\\frac{n\\epsilon^2}{2\\sigma^2}}$$\n",
    "\n",
    "This bound results in the upper confidence bound:\n",
    "\n",
    "$$ C_a(T_a(t),\\delta)= \\hat{\\mu}_a(t)+\\sqrt{\\frac{2\\sigma^2}{T_a(t)}\\log{\\frac{1}{\\delta}}},$$\n",
    "\n",
    "where $\\hat{\\mu}_a$ is the current sample mean for arm $a$:\n",
    "\n",
    "$$ \\hat{\\mu}_a =\\frac{1}{T_a(t)} \\sum_{i=1}^{T_a(t)} X_{a,i}$$\n",
    "\n",
    "We will once again choose the same time-varying $\\delta$ to ensure that we will always explore the arms.\n",
    "\n",
    "$$ \\delta(t)=\\frac{1}{t^2},$$\n",
    "\n",
    "resulting in the confidence bound:\n",
    "\n",
    "$$C_a(T_a(t),\\delta(t))= \\begin{cases} \\qquad \\qquad \\infty \\ \\qquad \\qquad &: T_a(t)=0 \\\\ \\hat{\\mu}_a(t)+\\sqrt{\\frac{4\\sigma^2}{T_a(t)}\\log{t}} \\ \\ \\ \\ \\ \\  &: T_a(t)>0\\end{cases}$$\n",
    "\n",
    "Now, use this confidence bound to fill out the following function which returns the choice of arm as well as the upper confidence bounds of each arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB_pull_arm(t,variance,times_pulled,rewards):\n",
    "    \"\"\" \n",
    "    Implement the choice of arm for the UCB algorithm\n",
    "    \n",
    "    Inputs:\n",
    "    iteration          - iteration of the bandit algorithm\n",
    "    times_pulled       - a list of length K (where K is the number of arms) of the number of \n",
    "                         times each arm has been pulled\n",
    "    rewards            - a list of K lists. Each of the K lists holds the samples received from pulling each arm up \n",
    "                         to iteration t. \n",
    "    \n",
    "    Returns:\n",
    "    arm                -  integer representing the arm that the UCB algorithm would choose.\n",
    "    confidence bounds  -  a list of the upper confidence bounds for each arm\n",
    "    \"\"\"\n",
    "\n",
    "    K=len(times_pulled)\n",
    "    delta=1.0/t**2\n",
    "    \n",
    "    confidence_bounds=[] #TODO\n",
    "    \n",
    "    arm=#TODO\n",
    "    \n",
    "    return arm , confidence_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function you have filled out, let us investigate the pseudo-regret of the UCB algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "UCB_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize Bandit_environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm using UCB algorithm\n",
    "        arm,confidence_bounds=UCB_pull_arm(t,variance,bandit_env.times_pulled,bandit_env.rewards)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "        \n",
    "    #Keep track of pseudo-regret  \n",
    "    UCB_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Make plot\n",
    "plt.plot(UCB_pseudo_regret/num_runs)\n",
    "plt.xlabel('#TODO')\n",
    "plt.ylabel('#TODO')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Compare the Algorithms\n",
    "Let us now compare the regret of the UCB and Chebyshev-UCB algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Plot\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Plot the pseudo-regrets of each algorithm\n",
    "plt.plot(chebyshev_pseudo_regret/num_runs ,label='#TODO')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='#TODO')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  D. Dependence on the Sub-optimality Gap\n",
    "Let us now compare the regret of the UCB algorithm on various instances with different minimum gaps between the optimal and sub-optimal arms. As we saw in Lecture, it should be much harder to find the best arm is the sub-optimal arms are very similar to it.\n",
    "\n",
    "We define the gap $\\Delta_a$ of a suboptimal arm $a=2,...,K-1$ as:\n",
    "\n",
    "$$ \\Delta_a=\\mu_1-\\mu_a$$\n",
    "\n",
    "We denote the minimum gap as:\n",
    "\n",
    "$$ \\Delta=\\min_{a>1} \\Delta_a$$ \n",
    "\n",
    "The following cell runs the UCB algorithm you have implemented on 5 UCB instances with decreasing minimum gaps:\n",
    "\n",
    "$$\\Delta=[4,2,1,0.5,0.25]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "# Defines the Gaps and means of each MAB instance we will be using\n",
    "gaps=[4,2,1,0.5,0.25]\n",
    "env_means=[[10,6,5,4,3,2],[10,8,7,6,5,4],[10,9,8,7,6,5],[10,9.5,8.5,7.5,6.5,5.5],[10,9.75,8.75,7.75,6.75,5.75]]\n",
    "\n",
    "\n",
    "for Delta,means in zip(gaps,env_means):\n",
    "    #Create bandit environment with the given means and standard deviations\n",
    "    env=BanditEnv(means,standard_deviations)\n",
    "    #Initialize Pseudo_regret\n",
    "    pseudo_regret=0\n",
    "    for run in range(num_runs):\n",
    "        #Initialize Bandit environment between runs\n",
    "        env.initialize(make_plot=0)\n",
    "        for t in range(1,T+1):\n",
    "            #Choose UCB arm\n",
    "            arm,confidence_bounds=UCB_pull_arm(t,variance,env.times_pulled,env.rewards)\n",
    "            #Aull arm\n",
    "            env.pull_arm(arm)\n",
    "        \n",
    "        #Log pseudo-regret\n",
    "        pseudo_regret+=np.array(env.regret)\n",
    "    \n",
    "    #Plot pseudo-regret for UCB on the given bandit instance\n",
    "    plt.plot(pseudo_regret/num_runs,label=r'$\\Delta$: {}'.format(Delta))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Algorithm\n",
    "If you want to visualize your algorithm, you can use the following interactive demo (If it is lagging, do not worry this part is not graded and is meant to build your intuition for the algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[9,8]\n",
    "\n",
    "# Creates an interactive bandit instance with an option to test your algorithm.\n",
    "#     - Pull an arm by clickling on the colored button\n",
    "#     - Allow your algorithm to choose the arm by clicking on the UCB button\n",
    "#     - The true means of the distributions are shown with the dashed horizontal lines\n",
    "#     - Large solid circle is the sample mean of the arm\n",
    "#     - Solid vertical line is the upper confidence bound you have calculated\n",
    "#     - The reward distribution of each arm is shown on the right and can be toggled on/off by checking the box\n",
    "#     - Running Pseudo-regret is shown on the bottom and can be toggled on/off by checking the box\n",
    "\n",
    "# You may need to rerun this cell to restart the gui\n",
    "alg=Interactive_UCB_Algorithm(bandit_env,UCB_pull_arm,'UCB')\n",
    "alg.run_Interactive_Alg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Bayesian Approach: Thompson Sampling\n",
    "The third algorithm we will analyze is the Bayesian take on multi-armed bandits, known as Thompson Sampling. In this setting, you begin with a prior over the mean of each arm $\\pi_a(\\mu_a)$.\n",
    "\n",
    "At each round $t=1,2...$, the algorithm computes the posterior probability $p_{a,t}$ that arm $a\\in \\mathcal{A}$ has the highest mean reward:\n",
    "\n",
    "$$ p_{a,t}=\\mathbb{P}\\bigg(\\mu_a=\\max_{a'} \\mu_{a'} \\ \\bigg| \\ X_{1,A_1},...,X_{t-1,A_{t-1}}\\bigg).$$\n",
    "\n",
    "The choice of arm is then randomly sampled from the distribution $p_t$ over $\\mathcal{A}$, where each arm $a\\in \\mathcal{A}$ has probsbility $p_{a,t}$:\n",
    "\n",
    "$$ A_t \\sim p_t $$ \n",
    "\n",
    "## Implementing Thompson Sampling\n",
    "\n",
    "Since the posterior distribution over each arm having the maximum mean is often intractable to compute, in practice we often implement a simpler algorithm that nevertheless accomplishes the same task.\n",
    "\n",
    "At each rount $t=1,2....$, you keep track of the posterior distribution over $\\mu_a$, for each arm $a \\in \\{ 0,1,...,K-1\\}$,  given all the samples you have observed from that arm $X_{a,1},...,X_{a,T_a(t-1)}$:\n",
    "\n",
    "$$P_{a,t}=\\mathbb{P}(\\mu_a | X_{a,1},...,X_{a,T_a(t-1)}).$$\n",
    "\n",
    "You then take one sample from $P_{a,t}$ and choose the arm with the highest sample:\n",
    "\n",
    "1. $\\mu_{a,t} \\sim  P_{a,t}$ for $a \\in \\{0,1,...,K-1\\}$.\n",
    "2. Choose arm:\n",
    "$$ A_t=\\underset{a \\in \\{ 0,1,...,K-1\\}}{\\operatorname{argmax}} \\mu_{a,t}$$\n",
    "\n",
    "Since the reward distributions in this lab are Gaussians with known variance $\\sigma_a^2$, we know from our investigation of conjugate priors that if we have Gaussian priors: $\\pi_a(\\mu_a)=\\mathcal{N}(\\mu_{a,0},\\sigma_{a,0}^2)$, the posterior distribution for each arm will also be a Gaussian. \n",
    "\n",
    "Therefore, to implement Thompson Sampling in this lab, the posterior distributions for each arm in this lab at each time $t=1,2,...$ are given by:\n",
    "\n",
    "$$ P_{a,t}=\\mathcal{N}(\\hat\\mu_{a,t},\\hat{\\sigma}_{a,t}^2)$$\n",
    "\n",
    "where,\n",
    " $$\\hat{\\sigma}_{a,t}^2 =\\bigg(\\frac{1}{\\sigma_{a,0}^2}+\\frac{T_a(t-1)}{\\sigma_a^2}\\bigg)^{-1} $$\n",
    "$$ \\hat\\mu_{a,t}=\\hat{\\sigma}_{a,t}^2 \\bigg( \\frac{\\mu_{a,0}}{\\sigma_{a,0}^2}+\\frac{\\sum_{i=1}^{T_a(t-1)} X_{a,i}}{\\sigma_a^2} \\bigg)$$\n",
    "\n",
    "Fill out the following function that implements the choice of arm for the Thompson Sampling algorithm with Gaussian arms and prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_pull_arm(t,variance,times_pulled,rewards,prior_means,prior_variances):\n",
    "    \"\"\" \n",
    "    Implement the choice of arm for the Thompson Sampling Algorithm when the arms and priors are Gaussians.\n",
    "    \n",
    "    Inputs:\n",
    "    iteration          - iteration of the bandit algorithm.\n",
    "    times_pulled       - a list of length K (where K is the number of arms) of the number of \n",
    "                         times each arm has been pulled.\n",
    "    rewards            - a list of K lists. Each of the K lists holds the samples received from pulling each arm up \n",
    "                         to iteration t.\n",
    "    prior_means        - a list of length K with the mean of the priorsfor each arm.\n",
    "    prior_variances    - a list of length K with the variance of the prior for each arm.\n",
    "    \n",
    "    Returns:\n",
    "    arm                - integer representing the arm that the UCB algorithm would choose.\n",
    "    posterior_samples  - list of samples from the posterior used to choose the arm. \n",
    "    posterior_means    - list of means of the posterior for each arm\n",
    "    posterior_vars     - list of variances of the posteriors of each arm\n",
    "    \"\"\"\n",
    "\n",
    "    K=len(times_pulled)\n",
    "    \n",
    "    posterior_samples=[] #TODO\n",
    "    posterior_means=[]   #TODO\n",
    "    posterior_vars=[]    #TODO       \n",
    "    arm=                 #TODO\n",
    "    \n",
    "    return arm , posterior_samples , posterior_means,posterior_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Thompson Sampling with Good Priors\n",
    "As we saw in class, the performance of Thompson Sampling can vary drastically with the quality of the prior. \n",
    "\n",
    "First, let us analyze the performance of Thompson Sampling when the priors reflect the correct rankings of the arms (meaning that the prior mean for arm $0$ is the highest). We will compare it to the performance of the UCB algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define Prior Means and Variances\n",
    "prior_means=[12,9,8,7,4,3,2]\n",
    "prior_vars=[2.2,2.2,2.2,2.2,2.2,2.2]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,variance,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Plot Thompson Sampling vs. UCB regret\n",
    "plt.plot(TS_pseudo_regret/num_runs ,label='TS Regret')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='UCB Regret')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Thompson Sampling with Bad Priors\n",
    "Now let us analyze the performance of Thompson Sampling when the priors have completely incorrect correct rankings of the arms, meaning that the prior mean for arm $0$ is the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define prior means and standard deviations\n",
    "prior_means=[2,3,4,5,6,7]\n",
    "prior_vars=[2.2,2.2,2.2,2.2,2.2,2.2]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Chosoe arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,variance,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Plot Thompson Sampling vs. UCB regret\n",
    "plt.plot(TS_pseudo_regret/num_runs ,label='TS Regret')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='UCB Regret')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Thompson Sampling with the same prior for each arm\n",
    "Now let us analyze the performance of Thompson Sampling when the priors are the same for all arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define prior means and variances\n",
    "prior_means=[8,8,8,8,8,8]\n",
    "prior_vars=[2.5,2.5,2.5,2.5,2.5,2.5]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=0\n",
    "for runs in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Chosoe arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,variance,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret+=np.array(bandit_env.regret)\n",
    "    \n",
    "#Plot Thompson Sampling vs. UCB regret\n",
    "plt.plot(TS_pseudo_regret/num_runs ,label='TS Regret')\n",
    "plt.plot(UCB_pseudo_regret/num_runs ,label='UCB Regret')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Pseudo-Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Your Algorithm\n",
    "If you want to visualize your algorithm, you can use the following interactive demo (If it is lagging, do not worry this part is not graded and is meant to build your intuition for the algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[9,8]\n",
    "\n",
    "# Creates an interactive bandit instance with an option to test your algorithm.\n",
    "#     - Pull an arm by clickling on the colored button.\n",
    "#     - Allow your algorithm to choose the arm by clicking on the UCB button.\n",
    "#     - The true means of the distributions are shown with the dashed horizontal lines.\n",
    "#     - Large solid circle is the sample mean of the rewards for the arm.\n",
    "#     - Solid vertical line shows the 95% credible interval for the arm.\n",
    "#     - The reward distribution of each arm is shown on the right and can be toggled on/off by checking the box.\n",
    "#     - Running Pseudo-regret is shown on the bottom and can be toggled on/off by checking the box.\n",
    "\n",
    "# You may need to rerun this cell to restart the gui\n",
    "alg=Interactive_TS_Algorithm(bandit_env,TS_pull_arm,'TS',prior_means,prior_vars)\n",
    "alg.run_Interactive_Alg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pros and Cons of UCB and Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, write a few sentences comparing and contrasting Chebyshev-UCB, UCB, and Thompson Sampling. What are some pros and cons of UCB and of Thompson Sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
