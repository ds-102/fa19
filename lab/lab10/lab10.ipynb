{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Classification Forests\n",
    "Welcome to the tenth DS102 lab! \n",
    "\n",
    "The goals of this lab is to implement and gain a hands on understanding of classification tree and forests.\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Tuesday November 26, 2019 at 11:59 PM.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Download the Data and Separate a Holdout Set \n",
    "\n",
    "In this lab, we will classify images of handwritten digits using classification trees and forests. First, we need to download the dataset of images of handwritten digits, matched with the label of the digit.\n",
    "\n",
    "Download the digits dataset from sklearn.datasets.load_digits(). Before doing any visualization or model training, separate a holdout set of 20% of the data drawn i.i.d. from the full dataset. Put the remaining instances in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "digits = datasets.load_digits()\n",
    "# total number of image - label instances\n",
    "n = len(digits['data'])\n",
    "\n",
    "# split into training set and holdout set.\n",
    "frac_holdout = .20\n",
    "\n",
    "rs = np.random.RandomState(0)\n",
    "n_holdout = int(n*frac_holdout)\n",
    "\n",
    "# randomly shuffle indices, then use the lat n_holdout instances for the holdout set\n",
    "random_idxs = rs.choice(n,n, replace=False)\n",
    "train_set_idxs = random_idxs[:-n_holdout]\n",
    "holdout_set_idxs = random_idxs[-n_holdout:]\n",
    "\n",
    "X_train = # TODO\n",
    "y_train = # TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "Now, visualize the training set data. For each class in [0,...,9], plot 3 images corresponding to images for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few training set image from each class\n",
    "num_from_each_class = 3\n",
    "fig, ax = plt.subplots(num_from_each_class,10,figsize=(10,3))\n",
    "for i in range(10):\n",
    "    ax[0,i].set_title('{0}'.format(i))\n",
    "    ids_with_label_i = # TODO: find the indices where the label is i.\n",
    "    \n",
    "    for j in range(num_from_each_class):\n",
    "        ax[j,i].imshow(X_train[ids_with_label_i[j]].reshape(8,8), cmap='gray')\n",
    "        ax[j,i].set_xticks([]), ax[j,i].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training a Classification Tree.\n",
    "\n",
    "Now we're ready to learn a predictor of the labels for each image, given instances from the training set. We'll use 5-fold cross validation on the training set to separate 5 train-validation set splits.  \n",
    "\n",
    "Fill in the function below to train the classication tree. Note that a tree is just a forest with only one estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform k-fold validation on the training set.\n",
    "def train_and_evaluate_tree(X, y, max_depths, num_folds=5):\n",
    "    # returns the per-fold validation accuracies for each setting of max_depths\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "\n",
    "    tree_accs_by_fold = np.zeros((num_folds, len(max_depths)))\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        for j, depth in enumerate(max_depths):\n",
    "            # fit_random_forest_classifier\n",
    "            # a tree is just a forest with one tree\n",
    "            clf = RandomForestClassifier(n_estimators=1, \n",
    "                                     max_depth=depth,\n",
    "                                     random_state=0)\n",
    "            # Fit the random tree\n",
    "            clf.fit(X_train, y_train)    \n",
    "            # predict using the fitted tree\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "        \n",
    "            # store accuracy for this fold and hyperparameter setting.\n",
    "            tree_accs_by_fold[i,j] = metrics.accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "    return tree_accs_by_fold\n",
    "\n",
    "\n",
    "# TODO: run this cell to train a single tree with max_depth 5\n",
    "print(train_and_evaluate_tree(X_train, y_train, max_depths= [5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll investigate how the accuracy of our classifier varies as we increase the complexity of the tree. \n",
    "In the cell below, compute the accuracies for 5-folds with the following max_depth settings: 1,2,4,8,16,32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot performance as a function of depth of the tree\n",
    "max_depths = # TODO\n",
    "accs_tree_by_depth = train_and_evaluate_tree(X_train, y_train, max_depths= max_depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each hyperparameters setting, plot the average, min, and max accuracy over the 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average, min, and max per-fold performance over all max_depth settings\n",
    "avgs_by_depth = np.average(accs_tree_by_depth, axis=0)\n",
    "mins_by_depth = # TODO\n",
    "maxs_by_depth = # TODO\n",
    "\n",
    "\n",
    "y_bounds_by_depth =  np.vstack((avgs_by_depth - mins_by_depth, maxs_by_depth - avgs_by_depth))                   \n",
    "plt.errorbar(x = max_depths,\n",
    "             y = avgs_by_depth,\n",
    "             yerr = y_bounds_by_depth)\n",
    "\n",
    "plt.xlabel('TODO')\n",
    "plt.ylabel('TODO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: What do you notice about the dependence of accuracy on the depth of the tree? What about the variation across folds? Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training Classification Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train classification forests with many trees. Fill in the following function to run k-fold cross validation to report the accuracy of different hyperparameters settings on the validation sets. Your implementation should look similar to the train_and_evaluate_tree() function above, where now we also loop over the number of trees in the forest as an additional hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_forest(X, y, max_depths, num_trees, num_folds=5):\n",
    "    # returns the per-fold validation accuracies for each setting of max_depths and num_trees\n",
    "\n",
    "    accs_by_fold = np.zeros((num_folds, len(num_trees), len(depths)))\n",
    "\n",
    "    kf = KFold(n_splits=num_folds)\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        print(\"on fold {0} of {1}\".format(i, num_folds))\n",
    "        X_train, X_val = # TODO\n",
    "        y_train, y_val = # TODO\n",
    "\n",
    "        for j, num_tree in enumerate(num_trees):\n",
    "            for k, depth in enumerate(max_depths):\n",
    "                \n",
    "                # TODO: fit a random forest classifier\n",
    "                \n",
    "                # TODO: fit the forest to the data   \n",
    "                \n",
    "                # TODO: use the forest to predict                \n",
    "                \n",
    "                # TODO: store the accuracy of the predictions\n",
    "    \n",
    "    # return the accuracy of the predictions                            \n",
    "    return accs_by_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the random forest classifier with the following hyperparameters\n",
    "num_trees = [1,10,20,50,100,200,400]\n",
    "depths = [2, 4, 8]\n",
    "\n",
    "accs_by_fold_forest = train_and_evaluate_forest(X_train, y_train,\n",
    "                                                    max_depths = depths,\n",
    "                                                    num_trees = num_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the performance of the random forest as a function of the number of trees in the forest. Specifically, \n",
    "for each setting of num_trees, plot the average validation set accuracy across folds for the setting of max_depths which maximizes this accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first take the average over folds\n",
    "accs_avged_by_fold = np.average(accs_by_fold_forest,axis=0)\n",
    "# then, for each setting of num_trees, maximize over the settings of max_depth\n",
    "accs_by_num_trees = np.max(accs_avged_by_fold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report the setting of max_depth which maximizes the average per-fold accuracy, for each setting of num_trees.\n",
    "best_depths_by_num_trees = # TODO\n",
    "print(best_depths_by_num_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: what do you notice about the optimal depth for each setting of num_trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, plot the accuracy as a function of the number of trees in the forest.\n",
    "plt.plot(num_trees, accs_by_num_trees)\n",
    "plt.xlabel('TODO')\n",
    "plt.ylabel('TODO')\n",
    "plt.title(\"Performance across forest sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Holdout Accuracy.\n",
    "Now, use the optimal setting of the hyperparameters you found above to train a random forest on the entire training set. Report the resulting accuracy of applying this random forest to predict the holdout set instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout accuracy\n",
    "X_holdout = # TODO \n",
    "y_holdout = # TODO\n",
    "\n",
    "num_trees = # TODO\n",
    "depth = # TODO\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= num_trees, \n",
    "                                         max_depth=depth,\n",
    "                                         random_state=0)\n",
    "\n",
    "# fit the random forest to the entire training set\n",
    "clf.fit(# TODO )  \n",
    "# preidct on the holdout set\n",
    "y_pred_holdout = # TODO \n",
    "\n",
    "# compute the holdout test accuracy\n",
    "acc_test = # TODO\n",
    "print(\"holdout set accuracy is {0:.3f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Compare the test set accuracies with the validation set accuracies you found in the cross-validation for this setting of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Investigating Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll investigate the feature importances associated with each pixel in the images. \n",
    "\n",
    "So far, we've been treating the image pixels (8x8 image = 64 pixels) as the feature representation of the image. The random forest model in sklearn calculates these feature importances for you. There, the feature importances are calculated the decrease in gini impurity due to splits on that feature (pixel), averaged across the trees in the forest. \n",
    "\n",
    "To visualize these differences effectively, we'll transform the problem into a binary classification problem between two digits. We chose digits 2 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into just intances of 2's and 7's\n",
    "y_bin_27 = y_train[np.where((y_train == 2) ^ (y_train == 7))]\n",
    "X_bin_27 = X_train[np.where((y_train == 2) ^ (y_train == 7))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit using the entire training set\n",
    "# use the values you found to be the best from kfold validation\n",
    "\n",
    "num_trees = 400\n",
    "depth = 8\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= num_trees, \n",
    "                                         max_depth=depth,\n",
    "                                         random_state=0)\n",
    "    \n",
    "clf.fit(X_bin_27, y_bin_27);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot what these images look like\n",
    "fig, ax = plt.subplots(1,10, figsize=(10,2))\n",
    "for i in range(10):\n",
    "    # shape the 64 features back into a 8x8 image\n",
    "    ax[i].imshow(X_bin_27[i].reshape(8,8), cmap='gray')\n",
    "    ax[i].set_title('{0}'.format(y_bin_27[i]))\n",
    "    ax[i].set_xticks([]), ax[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances for this binary classification problem\n",
    "plt.imshow(clf.feature_importances_.reshape(8,8))\n",
    "plt.colorbar(), plt.xticks([]), plt.yticks([])\n",
    "plt.title(\"feature importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: where in the image are the feature importances the highest? Where are they the lowest? Give a possible explanation for your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
