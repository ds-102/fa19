{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Q-learning\n",
    "Welcome to the ninth DS102 lab! \n",
    "\n",
    "The goals of this lab is to implement and gain a better understanding of Q-learning.\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Tuesday November 19, 2019 at 11:59 PM.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GridWorld class.\n",
    "We begin by defining a class for the environment in which we will run Q-learning. This is the grid world from class where we can have both a stochastic or deterministic environment. In the stochastic case the robot will have a probability of 0.8 of going in the direction it's told to go an a probability of 0.1 of going in each direction orthogonal to the direction it's meant to go in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD_PROB = 0.8\n",
    "LEFT_PROB = 0.1\n",
    "RIGHT_PROB = 0.1\n",
    "BACKWARD_PROB = 0.0\n",
    "FIXED_PROB = 0.0\n",
    "class GridWorld():\n",
    "    \"\"\"The grid world class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grid : list of list of str\n",
    "        The starting representation of the world. A single element\n",
    "        must be \"R\" which represents the starting location of the robot.\n",
    "        Any element that is \"\" represents a cell on which the robot can travel,\n",
    "        \"X\" represents a rock which the robot can not travel on, and any\n",
    "        cell with a string that can be converted to a number represents\n",
    "        a terminal state with its corresponding reward.\n",
    "    stochastic : bool\n",
    "        Whether the environment is stochastic or deterministic.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, grid, stochastic):\n",
    "        self._grid = grid\n",
    "        self.num_rows = len(grid)\n",
    "        self.num_cols = len(grid[0])\n",
    "        self._stochastic = stochastic\n",
    "        # Determine the starting location of the robot.\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                if self._grid[i][j] == \"R\":\n",
    "                    self._grid[i][j] = \"\"\n",
    "                    self._row_pos = i\n",
    "                    self._col_pos = j\n",
    "                    self._start_row_pos = i\n",
    "                    self._start_col_pos = j\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its original state.\"\"\"\n",
    "        self._row_pos = self._start_row_pos\n",
    "        self._col_pos = self._start_col_pos\n",
    "        return (self._row_pos, self._col_pos)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Move the robot a single step in the world.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : str\n",
    "            The desired direction to travel in. Can either be\n",
    "            \"north\", \"west\", \"east\", \"south\".\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pos : tuple of int\n",
    "            The location the robot ends up at after taking a step.\n",
    "            The first element represents the row and the second element\n",
    "            represents the column.\n",
    "        reward : float\n",
    "            The reward from taking this step.\n",
    "        done : bool\n",
    "            Whether the robot has reached a terminal state or not.\n",
    "\n",
    "        \"\"\"\n",
    "        # Determine the transition probabilities based on the action and\n",
    "        # whether the environment is stochastic or deterministic.\n",
    "        if self._stochastic:\n",
    "            if action == \"north\":\n",
    "                transition_probs = {\n",
    "                    \"north\": FORWARD_PROB,\n",
    "                    \"west\": LEFT_PROB,\n",
    "                    \"east\": RIGHT_PROB,\n",
    "                    \"south\": BACKWARD_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"west\":\n",
    "                transition_probs = {\n",
    "                    \"north\": RIGHT_PROB,\n",
    "                    \"west\": FORWARD_PROB,\n",
    "                    \"east\": BACKWARD_PROB,\n",
    "                    \"south\": LEFT_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"east\":\n",
    "                transition_probs = {\n",
    "                    \"north\": LEFT_PROB,\n",
    "                    \"west\": BACKWARD_PROB,\n",
    "                    \"east\": FORWARD_PROB,\n",
    "                    \"south\": RIGHT_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"south\":\n",
    "                transition_probs = {\n",
    "                    \"north\": BACKWARD_PROB,\n",
    "                    \"west\":RIGHT_PROB,\n",
    "                    \"east\": LEFT_PROB,\n",
    "                    \"south\": FORWARD_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "        else:\n",
    "            transition_probs = {\n",
    "                \"north\": 0.0,\n",
    "                \"west\": 0.0,\n",
    "                \"east\": 0.0,\n",
    "                \"south\": 0.0,\n",
    "                \"fixed\": 0.0\n",
    "            }\n",
    "            transition_probs[action] = 1.0\n",
    "            \n",
    "        # Account for the cases where we are on the boundaries or\n",
    "        # next to a rock.\n",
    "        row = self._row_pos\n",
    "        col = self._col_pos\n",
    "        if row == 0 or self._grid[row - 1][col] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"north\"]\n",
    "            transition_probs[\"north\"] = 0.0\n",
    "        if col == 0 or self._grid[row][col - 1] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"west\"]\n",
    "            transition_probs[\"west\"] = 0.0\n",
    "        if row == self.num_rows - 1 or self._grid[row + 1][col] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"south\"]\n",
    "            transition_probs[\"south\"] = 0.0\n",
    "        if col == self.num_cols - 1 or self._grid[row][col + 1] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"east\"]\n",
    "            transition_probs[\"east\"] = 0.0\n",
    "\n",
    "        # Decide which direction the robot will go.\n",
    "        directions = list(transition_probs.keys())\n",
    "        probs = list(transition_probs.values())\n",
    "        move = np.random.choice(directions, p=probs)\n",
    "        if move == \"north\":\n",
    "            self._row_pos -= 1\n",
    "        elif move == \"west\":\n",
    "            self._col_pos -= 1\n",
    "        elif move == \"east\":\n",
    "            self._col_pos += 1\n",
    "        elif move == \"south\":\n",
    "            self._row_pos += 1\n",
    "\n",
    "        # Check if we are on a final state and determine the reward.\n",
    "        if self._grid[self._row_pos][self._col_pos] != \"\":\n",
    "            reward = float(self._grid[self._row_pos][self._col_pos])\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "            \n",
    "        return (self._row_pos, self._col_pos), reward, done\n",
    "            \n",
    "    def render(self):\n",
    "        \"\"\"Print an ASCII visualization of the world.\"\"\"\n",
    "        for i, row in enumerate(self._grid):\n",
    "            row_strs = []\n",
    "            for j, elt in enumerate(row):\n",
    "                sys.stdout.write(\" -----\")\n",
    "                if i == self._row_pos and j == self._col_pos:\n",
    "                    elt = \"R\"\n",
    "                row_strs.append(elt.center(5))\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write(\"|\" + \"|\".join(row_strs) + \"|\")\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        for _ in range(self.num_cols):\n",
    "            sys.stdout.write(\" -----\")\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "In this section we will implement Q-learning for the grid world environment defined above. Recall that the optimal Q-function at a given state $s$ for an action $a$ is defined as\n",
    "$$Q(s, a) = \\sum_{s'} T(s, a, s')\\left[R(s, a, s') + \\gamma \\max_{a'} Q(s', a')\\right]$$\n",
    "where $\\gamma$ is the discount factor, $T(s, a, s')$ is the state transition probability function, and $R(s, a, s')$ is the reward function.\n",
    "\n",
    "Furthermore recall that we can learn the Q-function by updating our estimate of the optimal Q-function by averaging over the states and actions we observe. For example say we have some estimate of the Q-function $\\hat{Q}_k$ after observing $k$ samples, and say we observe a new sample which consists of $s$ the state we were at, $a$ the action we performed, $s'$ the state we ended up at, and $r$ the reward we got. Then our updated $Q$ function is given by\n",
    "$$\\hat{Q}_{k + 1}(s, a) \\leftarrow (1 - \\alpha)\\hat{Q}_k(s, a) + \\alpha \\left[r + \\gamma\\max_{a'} \\hat{Q}_k(s', a')\\right]$$\n",
    "where $\\alpha$ is a parameter between $0$ and $1$ that we set.\n",
    "\n",
    "Given this goal fill in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Q_values, old_state, action, new_state, reward, gamma, alpha):\n",
    "    \"\"\"Given an old estimate of the Q-function compute a new estimate\n",
    "    inplace by using observed samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_values : dict of dict\n",
    "        The estimate of the optimal Q values. The first index is over states\n",
    "        while the second index is over actions. So for example\n",
    "        Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "        (1, 2) and with action \"north\".\n",
    "    old_state : tuple of int\n",
    "        The state we were previously at before making the given action. The\n",
    "        first index represents the row while the second index represents the\n",
    "        column of the state.\n",
    "    action : string\n",
    "        The action we made. Can either be \"north\", \"east\", \"west\" or \"south\".\n",
    "    new_state : tuple of int\n",
    "        The state we transitioned to after making the given action.\n",
    "    reward : float\n",
    "        The reward we obtained after making our action.\n",
    "    gamma : float\n",
    "        The discount factor for the Q-function.\n",
    "    alpha : float\n",
    "        The proportion that tells us how we will weigh new incoming estimates of Q.\n",
    "\n",
    "    \"\"\"\n",
    "    # First compute the maximum Q-value at the new state.\n",
    "    max_Q = # TODO: Fill in.\n",
    "\n",
    "    # Now update the new Q value estimates.\n",
    "    Q_values[old_state][action] = # TODO: Fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define two types of agents. Ones that will always pick the best estimate of the Q-function and ones that will, with probability $\\epsilon$, pick a random action. The former is called a greedy agent while the latter is called an $\\epsilon$-greedy agent. We will explore why epsilon greedy agents are sometimes useful. Fill in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action():\n",
    "    \"\"\"Return a random action.\"\"\"\n",
    "    return np.random.choice([\"north\", \"south\", \"east\", \"west\"])\n",
    "\n",
    "def init_Q(env):\n",
    "    \"\"\"Return initial Q-value estimates with all values 0.\"\"\"\n",
    "    Q_values = {}\n",
    "    for i in range(env.num_rows):\n",
    "        for j in range(env.num_cols):\n",
    "            Q_values[i, j] = {\n",
    "                \"north\": 0.0,\n",
    "                \"west\": 0.0,\n",
    "                \"east\": 0.0,\n",
    "                \"south\": 0.0\n",
    "            }\n",
    "    return Q_values\n",
    "    \n",
    "def run_agent(Q_values, env, num_rollouts, gamma=0.9, alpha=0.1, epsilon=0.0, render=False):\n",
    "    \"\"\"Run a Q-learning agent in a given environment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q_values : dict of dict\n",
    "        The Q value estimates to start from.\n",
    "    env : GridWorld\n",
    "        The environment in which to run the agent.\n",
    "    num_rollouts : int\n",
    "        The number of times we wish to reset the environment\n",
    "        to its original state.\n",
    "    gamma : float\n",
    "        The discount factor for the Q-function.\n",
    "    alpha : float\n",
    "        The proportion that tells us how we will weigh new incoming estimates of Q.\n",
    "    epsilon : float\n",
    "        The proportion of times the agent will randomly pick an action instead\n",
    "        of making the optimal move in terms of the current estimate Q-function.\n",
    "        If epsilon is set to 0 this corresponds to a greedy agent.\n",
    "    render : bool\n",
    "        Whether to print the environment as it goes through each iteration.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q_values : dict of dict\n",
    "        The learned Q values. The first index is over states\n",
    "        while the second index is over actions. So for example\n",
    "        Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "        (1, 2) and with action \"north\".\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(num_rollouts):\n",
    "        state = env.reset()\n",
    "        if render:\n",
    "            time.sleep(0.4)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "        done = False\n",
    "        samples = []\n",
    "        while not done:\n",
    "            if np.random.binomial(1, epsilon):\n",
    "                action = random_action()\n",
    "            else:\n",
    "                # Take the best action according to the Q-value estimate.\n",
    "                # If multiple values are equal, randomly chose between them.\n",
    "                best_actions = # TODO: Fill in.\n",
    "                action = np.random.choice(best_actions)\n",
    "            old_state = state\n",
    "            state, reward, done = env.step(action)\n",
    "            samples.append((old_state, action, state, reward))\n",
    "            if render:\n",
    "                time.sleep(0.4)\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "        # Update the Q-function using samples from this rollout.\n",
    "        # It is much more efficient to use samples in reverse chronological order.\n",
    "        for old_state, action, state, reward in reversed(samples):\n",
    "            update_Q(Q_values, old_state, action, state, reward, gamma, alpha)\n",
    "    return Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning in Practice\n",
    "Let's begin by considering the deterministic setting. We consider a two-path setting where the agent can either receive a small reward by following a short path or a large reward by following a long path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the world.\n",
    "env = GridWorld([[\"\",  \"\",  \"1000\",  \"X\"],\n",
    "                 [\"\",  \"X\", \"X\", \"X\"],\n",
    "                 [\"\",  \"R\", \"\",  \"1\"]], stochastic=False)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 100 rollouts.\n",
    "Q_values = run_agent(Q_values, env, 100, alpha=1.0, render=False)\n",
    "\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which reward did the agent go for? Why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to run a simple epsilon-greedy agent in this setting. Let's try setting epsilon to 0.5 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 100 rollouts.\n",
    "Q_values = # TODO: Fill in.\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did epsilon do here? Explain why it caused the observed behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the stochastic setting. We'll consider a setting where we have a bridge that leads to a high-value end-state. However crossing carries with it a risk of falling down the side of the bridge. Try to find a value of gamma that will lead the robot to try to cross the bridge and a value of gamma that will lead the robot to take the lower valued option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the world.\n",
    "env = GridWorld([[\"\",  \"\", \"\", \"\", \"\", \"\", \"\",  \"-10\", \"-10\", \"\"],\n",
    "                 [\"100\", \"\", \"\", \"\", \"\", \"\", \"R\", \"\",     \"\",     \"2000\"],\n",
    "                 [\"\",  \"\", \"\", \"\", \"\", \"\", \"\",  \"-10\", \"-10\", \"\"]], stochastic=True)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 1000 rollouts.\n",
    "Q_values = run_agent(Q_values,\n",
    "                     env,\n",
    "                     1000,\n",
    "                     epsilon=0.1,\n",
    "                     gamma=# TODO: Fill in\n",
    "                     alpha=0.1,\n",
    "                     render=False)\n",
    "\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What values of gamma did you use to achieve each behavior? Why did these values work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill in. You don't need to include both gammas you used in the code cell above just mention them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Q-learning\n",
    "In `run_agent` we wait until the end of each rollout before we update the Q-function. Furthermore we do so in a reverse chronological order. Would updating the Q-function each time we see a new sample be better? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
