{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Metropolis Hastings Sampling\n",
    "Welcome to the fifth DS102 lab! \n",
    "\n",
    "The goals of this lab is to get a better understanding of the Metropolis Hastings sampling algorithm.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Wednesday October 9, 2019 at 11:59 PM.** This is intentionally one day later than usual since the homework is due on Tuesday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write collaborator names here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Function to Approximate\n",
    "In this lab we will be interested in approximating the simple function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ using Metropolis Hastings.\n",
    "This is mostly for the purpose of visualization, in real life we are usually interested in functions $f : \\mathbb{R^d} \\rightarrow \\mathbb{R}$ where $d$ is very large since this is where these MCMC type algorithms shine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_f(x):\n",
    "    \"\"\"A simple function from R to R we wish to approximate.\"\"\"\n",
    "    if x < 0 or x >= 4 * np.pi:\n",
    "        return 0\n",
    "\n",
    "    return 2 + np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Metropolis Hastings\n",
    "Now let's define some utility functions our Metropolis Hastings algorithm will use.\n",
    "\n",
    "Recall that Metropolis Hastings views the sampling process as sequential, that is our next sample is going to depend on our previous sample. This is done through a proposal distribution $v(x'|x)$, which given the previous sample $x$ defines a probability distribution over the new sample $x'$. In this case let's pick a Gaussian proposal distribution:\n",
    "$$v(x'|x) = \\mathcal{N}(x'; x, \\sigma),$$\n",
    "we see that the distribution has mean $x$ and standard deviation $\\sigma$ which we will pick later.\n",
    "\n",
    "Recall also that we don't always accept the new proposed sample, there's a chance our new sample is simply going to be set to our old sample $x' = x$. The probability $\\alpha$ of us accepting the new sample is given by\n",
    "$$\\min{\\left(1, \\frac{f(x')v(x|x')}{f(x)v(x'|x)}\\right)},$$\n",
    "where $f$ is the function we wish to approximate.\n",
    "\n",
    "Below we've implemented most functions but have left the function that computes $\\alpha$ empty for you to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_proposal_distribution(x_prime, x, sigma):\n",
    "    \"\"\"Given the previous sample x and new sample x_prime, computes v(x_prime|x).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_prime : float\n",
    "        The new sample at which we are evaluating the pdf.\n",
    "    x : float\n",
    "        The previous sample on which we are conditioning.\n",
    "    sigma : float\n",
    "        The standard deviation of our proposal function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    v : float\n",
    "        The value of v(x_prime|x).\n",
    "    \"\"\"\n",
    "    return scipy.stats.norm.pdf(x_prime, loc=x, scale=sigma)\n",
    "\n",
    "def draw_from_proposal_distribution(x, sigma):\n",
    "    \"\"\"Given the previous sample x, draws a new sample from v(x_prime|x).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : float\n",
    "        The previous sample on which we are conditioning.\n",
    "    sigma : float\n",
    "        The standard deviation of our proposal function.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_prime : float\n",
    "        The generated proposed next state.\n",
    "    \"\"\"\n",
    "    return np.random.normal(x, sigma)\n",
    "\n",
    "def accept_probability(x_prime, x, sigma):\n",
    "    \"\"\"Computes the probability of accepting a new proposed sample given the previous sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_prime : float\n",
    "        The new proposed sample.\n",
    "    x : float\n",
    "        The previous sample from which we generated the new sample.\n",
    "    sigma : float\n",
    "        The standard deviation of our proposal function.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    alpha : float\n",
    "        The probability we will accept the proposed sample.\n",
    "    \"\"\"\n",
    "    frac = # TODO: Fill this in.\n",
    "    return np.minimum(1, frac)\n",
    "\n",
    "def true_with_probability_alpha(alpha):\n",
    "    \"\"\"Randomly returns True with probability alpha, otherwise returns False.\"\"\"\n",
    "    return np.random.binomial(1, alpha) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Metropolis Hastings\n",
    "Now that we've defined all our utility functions we can actually implement Metropolis Hastings. We've implemented most of the algorithm for you. You only need to fill in the TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(num_draws, burn_in_steps, sample_frequency, sigma):\n",
    "    \"\"\"Approximate objective_f using metropolis hastings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_draws : int\n",
    "        The total number of samples drawn from our proposal function after burn-in.\n",
    "    burn_in_steps : int\n",
    "        The number of initial burn-in steps before we start considering samples.\n",
    "    sample_frequency : int\n",
    "        How many steps we should wait before we save the current sample.\n",
    "    sigma : float\n",
    "        The standard deviation of our proposal function.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    samples : array of floats\n",
    "        All the samples we've saved throughout the algorithm.\n",
    "    all_points : array of floats\n",
    "        All proposed samples (including the burn-in period), even the ones we\n",
    "        didn't save.\n",
    "    \"\"\"\n",
    "    # Initialize our return arrays.\n",
    "    samples = []\n",
    "    all_points = []\n",
    "    \n",
    "    # Initialize the first sample to be 3.\n",
    "    x = 3\n",
    "    for i in range(num_draws + burn_in_steps):\n",
    "        all_points.append(x)\n",
    "        # Propose a new point.\n",
    "        x_prime = # TODO: Fill this in.\n",
    "        # Compute its probability of being accepted.\n",
    "        alpha = accept_probability(x_prime, x, sigma)\n",
    "        # Decide whether to accept the point.\n",
    "        if true_with_probability_alpha(alpha):\n",
    "            x = x_prime\n",
    "        # Decide whether to save the point.\n",
    "        if i > burn_in_steps and i % sample_frequency == 0:\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples), np.array(all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Metropolis Hastings\n",
    "Now that we have our algorithm let's see how well it approximates the objective function. We will try running Metropolis Hastings with four different values of $\\sigma$ to see how it affects the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_total = []\n",
    "all_points_total = []\n",
    "sigmas = [0.01, 1.0, 100.0, 10000.0]\n",
    "for sigma in sigmas:\n",
    "    samples, all_points = metropolis_hastings(num_draws=10000,\n",
    "                                              burn_in_steps=1000,\n",
    "                                              sample_frequency=10,\n",
    "                                              sigma=sigma)\n",
    "    samples_total.append(samples)\n",
    "    all_points_total.append(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the paths we take across the samples space for the different values of sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8), nrows=4)\n",
    "\n",
    "idxs = np.arange(all_points_total[0].shape[0])\n",
    "for sigma, all_points, row in zip(sigmas, all_points_total, ax):\n",
    "    row.plot(idxs, all_points, zorder=2)\n",
    "    row.set_title(\"$\\sigma$=\" + str(sigma))\n",
    "    row.set_xlabel(\"TODO: Fill this in.\")\n",
    "    row.set_ylabel(\"TODO: Fill this in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we approximated the true function for each value of sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,16), nrows=4)\n",
    "\n",
    "for sigma, samples, row in zip(sigmas, samples_total, ax):\n",
    "    # Plot a histogram of all the samples we accepted.\n",
    "    row.hist(samples, bins=100, density=True)\n",
    "    # Now plot a curve of the true function, normalized to be a valid pdf, as a baseline.\n",
    "    idxs = np.linspace(0, 4 * np.pi, num=1000)\n",
    "    norm_const= 1 / (2 * 4 * np.pi)\n",
    "    row.plot(idxs, [objective_f(i) * norm_const for i in idxs])\n",
    "    \n",
    "    row.set_title(\"$\\sigma=$\" + str(sigma))\n",
    "    row.set_ylabel(\"TODO: Fill this in.\")\n",
    "plt.xlabel(\"TODO: Fill this in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitatively, which values of $\\sigma$ do you think performed best at approximating the objective function?\n",
    "Can you explain why we observe the four sampling paths in the first set of plots?\n",
    "How do you think performance is tied to the sampling paths we plotted above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill this in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
