{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Algorithms for Online FDR Control\n",
    "Welcome to the second DS102 lab! \n",
    "\n",
    "The goals of this lab is to understand the mechanics of the LORD algorithm, and why it controls FDR. You'll modify the provided code for LORD to match the simpler method that we saw in lecture, and you will assess the performance of both methods under various sequences of p-values.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Tuesday September 17, 2019 at 11:59 PM.**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a restatement of the lord algorithm provided in HW2. \n",
    "# Part 1a) will ask you to modify it to return alpha_ts.\n",
    "def LORD(stream,alpha):\n",
    "    # Inputs: stream - array of p-values, alpha - target FDR level\n",
    "    # Output: array of indices k such that the k-th p-value corresponds to a discovery,\n",
    "    #         array of alpha values for each time step\n",
    "    \n",
    "    gamma = lambda t: 6 / (math.pi * t) ** 2\n",
    "    w_0 = alpha / 2\n",
    "    rejections = []\n",
    "    alpha_t = gamma(1) * w_0\n",
    "    \n",
    "    n = len(stream)\n",
    "    \n",
    "    for t in range(1, n + 1):\n",
    "        # Offset by one since indexing by 1 for t.\n",
    "        p_t = stream[t - 1] \n",
    "        if p_t < alpha_t:\n",
    "            rejections.append(t)\n",
    "\n",
    "        next_alpha_t = gamma(t + 1) * w_0 + alpha * sum([gamma(t + 1 - rej) for rej in rejections])\n",
    "        # Check if tau_1 exists.\n",
    "        if len(rejections) > 0:\n",
    "            next_alpha_t -= gamma(t + 1 - rejections[0]) * w_0\n",
    "        \n",
    "        # Update alpha.\n",
    "        alpha_t = next_alpha_t\n",
    "    # Shift rejections since the rejections are 1-indexed.\n",
    "    shifted_rej = [rej - 1 for rej in rejections]\n",
    "    \n",
    "    # TODO: fill-in return alpha_ts in addition to shifted_reg.\n",
    "    return shifted_rej, alpha_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Tracking Decisions and Alphas Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) Modify the LORD algorithm above to return not only the decisions, but also the alpha_t's that correspond to each time step. \n",
    "\n",
    "Use the code below to plot the decisions and alpha values over time. Make sure to fill in all the missing labels in the plotting function defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the plotting code so that the relevant quantities are correctly labeled.\n",
    "def plot_disc_and_alphas(discoveries, alphas, algorithm_name, color='green', plotting_offset=0.5):\n",
    "    m = len(alphas)\n",
    "    plt.bar(np.arange(1, m + 1), alphas, label=algorithm_name, color=color, alpha=0.8)\n",
    "    \n",
    "    for i, disc in enumerate(discoveries):\n",
    "        plt.axvline(disc + 1 + plotting_offset, color=color)  \n",
    "        plt.annotate(r\"$\\tau{0}$\".format(i), (disc + plotting_offset, np.max(alphas)), color=color, size=16)\n",
    "    plt.xlabel(\"TODO: fill in\")\n",
    "    plt.ylabel('TODO: fill in')\n",
    "    \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP FOR PROBLEM 1 - Run this cell to instantiate the p-values.\n",
    "m = 20\n",
    "\n",
    "rs = np.random.RandomState(0)\n",
    "# p-values for the null generated from uniform (0,1).\n",
    "p_values= rs.uniform(0, 1, size=m)\n",
    "\n",
    "# Instantiate p-values for the alternative as 0.001.\n",
    "alt_idxs = [0, 1, 4, 5, 10]\n",
    "p_values[alt_idxs] = 0.001\n",
    "alpha = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the lord algorithm .\n",
    "discoveries, alphas = LORD(p_values, alpha)\n",
    "\n",
    "# Plot the discoveries (make sure everything is labeled).\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plot_disc_and_alphas(discoveries, alphas, algorithm_name=\"LORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Now make a second function, LORD_most_recent() that follows the algorithm we analyzed in class; that is, the update step on alpha only depends on the most recent discovery (rather than the sum over all previous discoveries).\n",
    "\n",
    "The update rule is given by\n",
    "$$\\alpha_t = \\begin{cases} \\gamma_t \\alpha, \\text{ if no rejection has yet been made}\\\\\n",
    "\\gamma_{t-r_t} \\alpha, \\text{ otherwise} \\end{cases}.$$\n",
    "\n",
    "As in part a we use\n",
    "$$\\gamma_t = \\frac{6}{(\\pi t)^2}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggestion: start by copying the LORD() function you modified above to output alphas. Then modify as needed.\n",
    "def LORD_most_recent(stream,alpha):\n",
    "    # Inputs: stream - array of p-values, alpha - target FDR level\n",
    "    # Output: array of indices k such that the k-th p-value corresponds to a discovery\n",
    "\n",
    "    return shifted_rej, alpha_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the p-values from before so we can compare the plots.\n",
    "# Run the LORD algorithm variant you defined above.\n",
    "discoveries_2, alphas_2 = LORD_most_recent(p_values, alpha)\n",
    "\n",
    "# Plot the discoveries from the original algorithm.\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# The plotting offset is so the discoveries don't overwrite themselves.\n",
    "plot_disc_and_alphas(discoveries, alphas, color=\"green\", algorithm_name=\"TODO: fill in\", plotting_offset=0.4)\n",
    "\n",
    "# Plot the discoveries from the variant.\n",
    "plot_disc_and_alphas(discoveries_2, alphas_2, color=\"blue\", algorithm_name=\"TODO: fill in\", plotting_offset=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) What do you notice? Are the two algorithms making substantially different decisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: fill in what you notice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Repeat the same experiment with a second set of p-values, defined in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP FOR PROBLEM 1\n",
    "m = 70\n",
    "\n",
    "rs = np.random.RandomState(0)\n",
    "# p-values for the null generated from uniform (0,1).\n",
    "p_values= rs.uniform(0, 1, size=m)\n",
    "\n",
    "# Instantiate p-values for the alternative as 0.001.\n",
    "alt_idxs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 15, 20, 25, 30, 35, 40, 50]\n",
    "p_values[alt_idxs] = 0.001\n",
    "alpha = 0.05\n",
    "\n",
    "# Run the algorithms.\n",
    "discoveries, alphas = LORD(p_values, alpha)\n",
    "discoveries_2, alphas_2 = LORD_most_recent(p_values, alpha)\n",
    "\n",
    "# Plot the results.\n",
    "# Make the figure very wide so you can see what's going on.\n",
    "fig, ax = plt.subplots(figsize=(30, 5))\n",
    "plot_disc_and_alphas(discoveries, alphas, color=\"green\", \n",
    "                     algorithm_name=\"TODO: fill in\", \n",
    "                     plotting_offset=0.4)\n",
    "plot_disc_and_alphas(discoveries_2, alphas_2, color=\"blue\",\n",
    "                     algorithm_name=\"TODO: fill in\", \n",
    "                     plotting_offset=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a) What do you notice? Compare with your results from the p-value sequence from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: fill in with your findings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
