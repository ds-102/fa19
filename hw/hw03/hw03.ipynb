{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets\n",
    "We begin by loading both the Boston and Iris datasets. More information about the Boston dataset can be found [here](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and more information about the Iris dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/iris). Do not change the cell below this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random state for reproducibility.\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Load and shuffle the Boston dataset. Only subsample some features.\n",
    "boston_X, boston_y = load_boston(return_X_y=True)\n",
    "permutation = random_state.permutation(boston_X.shape[0])\n",
    "boston_X = boston_X[permutation][:, [5, 6]]\n",
    "boston_y = boston_y[permutation]\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "boston_X_train = boston_X[:-100]\n",
    "boston_y_train = boston_y[:-100]\n",
    "boston_X_test = boston_X[-100:]\n",
    "boston_y_test = boston_y[-100:]\n",
    "\n",
    "# Create a new featurization for the boston dataset by turning the current\n",
    "# features into a tenth degree polynomial.\n",
    "boston_poly_X_train = PolynomialFeatures(8).fit_transform(boston_X_train)\n",
    "boston_poly_X_test = PolynomialFeatures(8).fit_transform(boston_X_test)\n",
    "\n",
    "# Now load and shuffle the Iris dataset.\n",
    "# Discarding all output labels that correspond to a 2.\n",
    "iris_X, iris_y = load_iris(return_X_y=True)\n",
    "iris_X = iris_X[:100]\n",
    "iris_y = iris_y[:100]\n",
    "permutation = random_state.permutation(iris_X.shape[0])\n",
    "iris_X = iris_X[permutation]\n",
    "iris_y = iris_y[permutation]\n",
    "\n",
    "# Split the dataset into train and test sets.\n",
    "iris_X_train = iris_X[:-20]\n",
    "iris_y_train = iris_y[:-20]\n",
    "iris_X_test = iris_X[-20:]\n",
    "iris_y_test = iris_y[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Prediction functions\n",
    "We now define the regression and prediction functions. You need to fill these out, we provide `logistic_cross_entropy_loss_gradient` as an example.\n",
    "\n",
    "Remember that the squared loss (with regularization) with respect to linear regression is defined as $\\|X\\beta - y\\|_2^2 + \\lambda \\|\\beta\\|_2^2$ where $\\beta$ is the linear model, $X$ is the feature matrix, $\\lambda$ is a regularization term, and $y$ are the true output values. Furthermore remember that the derivative of $\\|X\\beta - y\\|_2^2$ with respect to $\\beta$ is $2X^{\\top}(X\\beta - y)$ and the derivative of $\\lambda \\|\\beta\\|_2^2$ is $2\\lambda \\beta$.\n",
    "\n",
    "The cross entropy loss with respect to logistic regression is defined as $-\\frac{1}{n} \\sum_{i=1}^n \\left[y_i \\log \\sigma(x_i^{\\top}\\beta) + (1 - y_i)\\log\\sigma(-x_i^{\\top}\\beta)\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_predict(X, beta):\n",
    "    \"\"\"Given a linear model (aka a vector) and a feature matrix\n",
    "    predict the output vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array of shape n\n",
    "        The predicted output vector.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2b)\n",
    "    pass\n",
    "\n",
    "def regression_least_squares(X, true_y, lambda_value):\n",
    "    \"\"\"Compute the optimal linear model that minimizes the regularized squared loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vector.\n",
    "    lambda_value : float\n",
    "        A non-negative regularization term.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    beta : numpy array of shape d\n",
    "        The optimal linear model.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2a)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_predict(X, beta):\n",
    "    \"\"\"Given a linear model (aka a vector) and a feature matrix\n",
    "    predict the probability of the output label being 1 using logistic\n",
    "    regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : numpy array of shape n\n",
    "        The predicted output vector.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2b)\n",
    "    pass\n",
    "\n",
    "def logistic_cross_entropy_loss(X, beta, true_y):\n",
    "    \"\"\"Output the cross entropy loss of a given logistic model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors. Consists of 0s and 1s.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        The value of the loss.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2b)\n",
    "    pass\n",
    "\n",
    "def logistic_cross_entropy_loss_gradient(X, beta, true_y):\n",
    "    \"\"\"Output the gradient of the squared loss evaluated with respect to beta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    beta : numpy array of shape d\n",
    "        The linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss_gradient : numpy array of shape d\n",
    "        The gradient of the loss evaluated with respect to beta.\n",
    "    \"\"\"\n",
    "    return -np.sum((true_y - sigmoid(X @ beta)) * X.T, axis=1) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, init_beta, true_y, loss, loss_gradient,\n",
    "                     learning_rate, iterations):\n",
    "    \"\"\"Performs gradient descent on a given loss function and\n",
    "    returns the optimized beta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array of shape nxd\n",
    "        The feature matrix where each row corresponds to a single\n",
    "        feature vector.\n",
    "    init_beta : numpy array of shape d\n",
    "        The initial value for the linear model.\n",
    "    true_y : numpy array of shape n\n",
    "        The true output vectors.\n",
    "    loss : function\n",
    "        The loss function we are optimizing.\n",
    "    loss_gradient : function\n",
    "        The gradient function that corresponds to the loss function.\n",
    "    learning_rate : float\n",
    "        The learning rate for gradient descent.\n",
    "    iterations : int\n",
    "        The number of iterations to optimize the loss for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    beta : numpy array of shape d\n",
    "        The optimized beta.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in (Q2c)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models for the Boston dataset\n",
    "In the section below you will train a regression model and evaluate it against the RMSE for the Boston housing dataset we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in (Q2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model for the logistic regression dataset\n",
    "In this section you will train a logistic model and evaluate it against the MAE for the Iris dataset we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in (Q2f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
